{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0336bdb9e57cd699596144781d3896191cc7f291091104ee557bbaee3fcafe96c",
   "display_name": "Python 3.8.8 64-bit ('ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install allennlp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ft/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "from allennlp.modules.elmo import batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "# Load ELMo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "# weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "options_file = './downloads/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n",
    "weight_file = './downloads/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file=weight_file, dropout=0).to('cuda')"
   ]
  },
  {
   "source": [
    "# Common functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "\n",
    "    tokenized_senteces = list()\n",
    "    for s, sent in enumerate(sentences):\n",
    "\n",
    "        remove_punctuation = RegexpTokenizer(r\"\\w+\")\n",
    "        tokenized_sent = remove_punctuation.tokenize(sent)\n",
    "\n",
    "        for i, t in enumerate(tokenized_sent):\n",
    "            if t.isnumeric():\n",
    "                tokenized_sent[i] = \"<num>\"\n",
    "\n",
    "        tokenized_senteces.append(tokenized_sent)\n",
    "\n",
    "    return tokenized_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter(tokenized_sentences, thresh=5):\n",
    "    \n",
    "    words = list()\n",
    "    for t_sent in tokenized_sentences:\n",
    "        for word in t_sent:\n",
    "            words.append(word)\n",
    "\n",
    "    vocab = Vocabulary(words, unk_cutoff=thresh)\n",
    "\n",
    "    filtered_sentences = list()\n",
    "\n",
    "    for t, tokenized_sent in enumerate(tokenized_sentences):\n",
    "        filtered_sent = list()\n",
    "        for word in tokenized_sent:\n",
    "            if vocab.lookup(word) == '<UNK>':\n",
    "                filtered_sent.append('<unk>')\n",
    "            else: \n",
    "                filtered_sent.append(word)\n",
    "        filtered_sentences.append(filtered_sent)\n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embedding(filtered_sentences, device=device):\n",
    "    vecs = list()\n",
    "    batch_size = 128\n",
    "    batches = math.ceil( len(filtered_sentences) / batch_size )\n",
    "    for i in tqdm(range(batches)):\n",
    "        character_ids = batch_to_ids(filtered_sentences[i * batch_size : (i + 1) * batch_size])\n",
    "        batch_embedding = elmo_embedder(character_ids.to(device))\n",
    "        batch_embedding = torch.mean(batch_embedding, dim=1)\n",
    "        vecs.extend(batch_embedding.cpu().detach().numpy())\n",
    "\n",
    "    return np.array(vecs)"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_csv('./preprocessed_data/trn.csv', delimiter = ',', names=['title','label'])\n",
    "tst = pd.read_csv('./preprocessed_data/tst.csv', delimiter = ',', names=['title','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sentences = tokenize(list(trn.title))\n",
    "trn_filtered_sentences = token_filter(trn_sentences,5)\n",
    "\n",
    "tst_sentences = tokenize(list(tst.title))\n",
    "tst_filtered_sentences = token_filter(tst_sentences,5)"
   ]
  },
  {
   "source": [
    "# Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 246/246 [05:39<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "trn_embedings = get_elmo_embedding(trn_filtered_sentences, device)\n",
    "\n",
    "f = open(\"./preprocessed_embeddings/elmo_trn.pkl\",\"wb\")\n",
    "pickle.dump(trn_embedings,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 36/36 [00:45<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "tst_embedings = get_elmo_embedding(tst_filtered_sentences, device)\n",
    "\n",
    "f = open(\"./preprocessed_embeddings/elmo_tst.pkl\",\"wb\")\n",
    "pickle.dump(tst_embedings,f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "# Linear Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=200)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "classifier = LogisticRegression(fit_intercept=True, penalty=\"l2\", C=1, max_iter=200)\n",
    "classifier.fit(trn_embedings, trn.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training accuracy = 0.9984408311324657\nTest accuracy = 0.9944345503116652\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy = {}\".format(classifier.score(trn_embedings, trn.label)))\n",
    "print(\"Test accuracy = {}\".format(classifier.score(tst_embedings, tst.label)))"
   ]
  }
 ]
}