{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t2fq8wgxNfVu"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Sgjep_51NTaF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchtext libraries\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator, Dataset\n",
    "\n",
    "# huggingface libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lpY8IUtYNTaK"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zV2ogmpxNTaL"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voijfr7pNTaL"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ykqOx6TNTaL"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.long)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field), ('label', label_field)]\n",
    "train, valid, test = TabularDataset.splits(path='preprocessed_data/', train='trn.csv', validation='val.csv',\n",
    "                                           test='tst.csv', format='CSV', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4h9OU9HCNTaM"
   },
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=8, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=8, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=8, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lWKv873sNTaM"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, pred = self.encoder(text, labels=label)[:2]\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0A2wt9fjNTaN"
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 10,):\n",
    "    \n",
    "    # initialize values\n",
    "    running_loss = 0.0\n",
    "    total_acc = 0\n",
    "    # training loop\n",
    "    # model.train()\n",
    "    total_acc = 0\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        test_acc = 0\n",
    "        val_acc = 0\n",
    "        test_seen = 0\n",
    "        val_seen = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:   \n",
    "            text = batch.text\n",
    "            labels = batch.label\n",
    "            labels = labels.to(device)\n",
    "            text = text.to(device)\n",
    "            output = model(text, labels)\n",
    "            loss, pred = output\n",
    "            test_acc += torch.eq(pred, labels).sum().item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            test_seen += len(batch)\n",
    "        test_acc = test_acc/test_seen\n",
    "        print('The training accuracy for epoch {epoch} is {test_acc}'.format(epoch=epoch+1, test_acc=test_acc))\n",
    "        print('The cumulative loss for epoch {epoch} is {epoch_loss}'.format(epoch=epoch+1, epoch_loss=epoch_loss))\n",
    "        # validation \n",
    "        model.eval()\n",
    "        for batch in valid_loader:   \n",
    "            text = batch.text\n",
    "            labels = batch.label\n",
    "            labels = labels.to(device)\n",
    "            text = text.to(device)\n",
    "            output = model(text, labels)\n",
    "            loss, pred = output\n",
    "            val_acc += torch.eq(pred, labels).sum().item()\n",
    "            val_seen += len(batch)\n",
    "        val_acc = val_acc/val_seen\n",
    "        print('The validation accuracy for epoch {epoch} is {val_acc}'.format(epoch=epoch+1, val_acc=val_acc))\n",
    "        if(val_acc > best_val_acc):\n",
    "          best_val_acc = val_acc\n",
    "          torch.save(model.state_dict(), 'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3RpG1SQNTaN",
    "outputId": "b8a76d25-d743-4975-a554-0e99d4c207f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy for epoch 1 is 0.9678291860243111\n",
      "The cumulative loss for epoch 1 is 344.6585331477472\n",
      "The validation accuracy for epoch 1 is 0.757295611494765\n",
      "The training accuracy for epoch 2 is 0.9853942595303252\n",
      "The cumulative loss for epoch 2 is 174.31880762305082\n",
      "The validation accuracy for epoch 2 is 0.8841612831365561\n",
      "The training accuracy for epoch 3 is 0.9922039075924394\n",
      "The cumulative loss for epoch 3 is 104.5520895757436\n",
      "The validation accuracy for epoch 3 is 0.8713521942526176\n",
      "The training accuracy for epoch 4 is 0.9950995990581047\n",
      "The cumulative loss for epoch 4 is 63.46034166841855\n",
      "The validation accuracy for epoch 4 is 0.8586544887502785\n",
      "The training accuracy for epoch 5 is 0.9971997708903456\n",
      "The cumulative loss for epoch 5 is 38.07068478666588\n",
      "The validation accuracy for epoch 5 is 0.9253731343283582\n",
      "The training accuracy for epoch 6 is 0.9976452618850633\n",
      "The cumulative loss for epoch 6 is 30.28521613227167\n",
      "The validation accuracy for epoch 6 is 0.9355090220539095\n",
      "The training accuracy for epoch 7 is 0.9984089607331509\n",
      "The cumulative loss for epoch 7 is 18.67494049220113\n",
      "The validation accuracy for epoch 7 is 0.9573401648474048\n",
      "The training accuracy for epoch 8 is 0.9987271685865207\n",
      "The cumulative loss for epoch 8 is 17.591202769712254\n",
      "The validation accuracy for epoch 8 is 0.948540877701047\n",
      "The training accuracy for epoch 9 is 0.9985362438744988\n",
      "The cumulative loss for epoch 9 is 22.35239484736121\n",
      "The validation accuracy for epoch 9 is 0.9454221430162619\n",
      "The training accuracy for epoch 10 is 0.9988226309425317\n",
      "The cumulative loss for epoch 10 is 17.049210830557058\n",
      "The validation accuracy for epoch 10 is 0.9660280686121631\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "train(model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QXiIRSebwAAC"
   },
   "outputs": [],
   "source": [
    "# TODO ADD TIMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jVVpnVyRGQU",
    "outputId": "b70601f6-b3a5-44e1-f598-16fff3caeef3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "effNdcaFwAkh"
   },
   "outputs": [],
   "source": [
    "# TODO ADD PREDICT FUNCTION \n",
    "def predict(text, label, model):\n",
    "  model.eval()\n",
    "  text = tokenizer.encode(text)\n",
    "  text=torch.tensor(text)\n",
    "  text = text.unsqueeze(0)\n",
    "  label = torch.tensor(label)\n",
    "  label = label.unsqueeze(0)\n",
    "  text = text.to(device)\n",
    "  label = label.to(device)\n",
    "  model=model.to(device)\n",
    "  loss, pred = model(text, label)\n",
    "  return pred.item()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ABaLaKtzM13",
    "outputId": "516ff73f-4ba7-45d8-bd40-fed3a99528f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "text = 'Maine GOP Governor’s Statement On Drug Overdoses Proves ‘Pro-Life Republicans’ Don’t Exist'\n",
    "label = 0\n",
    "output = predict(text, label, model)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_news_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
