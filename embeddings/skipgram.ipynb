{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skipgram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "08_S3VGgs7u-"
      },
      "source": [
        "!pip install googledrivedownloader;\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ipX2QRwYms"
      },
      "source": [
        "def tokenize(sentences):\n",
        "\n",
        "    tokenized_senteces = list()\n",
        "    for s, sent in enumerate(sentences):\n",
        "\n",
        "        remove_punctuation = RegexpTokenizer(r\"\\w+\")\n",
        "        tokenized_sent = remove_punctuation.tokenize(sent)\n",
        "\n",
        "        for i, t in enumerate(tokenized_sent):\n",
        "            if t.isnumeric():\n",
        "                tokenized_sent[i] = \"<num>\"\n",
        "\n",
        "        tokenized_senteces.append(tokenized_sent)\n",
        "\n",
        "    return tokenized_senteces\n",
        "\n",
        "\n",
        "def token_filter(tokenized_sentences, thresh=5):\n",
        "    \n",
        "    words = list()\n",
        "    for t_sent in tokenized_sentences:\n",
        "        for word in t_sent:\n",
        "            words.append(word)\n",
        "\n",
        "    vocab = Vocabulary(words, unk_cutoff=thresh)\n",
        "    print(\"Length of vocab: {}\".format(len(vocab)))\n",
        "    filtered_sentences = list()\n",
        "\n",
        "    for t, tokenized_sent in enumerate(tokenized_sentences):\n",
        "        filtered_sent = list()\n",
        "        for word in tokenized_sent:\n",
        "            if vocab.lookup(word) == '<UNK>':\n",
        "                filtered_sent.append('<unk>')\n",
        "            else: \n",
        "                filtered_sent.append(word)\n",
        "        filtered_sentences.append(filtered_sent)\n",
        "\n",
        "    return filtered_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9_wpXWcrpxa",
        "outputId": "74fa5d23-4950-49dc-cb3b-1003d42345bd"
      },
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "\n",
        "def Skipgram(filtered_sentences, ws=3, dim=50):\n",
        "    model = fasttext.train_unsupervised(filtered_sentences, model='skipgram', ws=ws, dim=dim, neg=5)\n",
        "    vocab_dict = {} \n",
        "    vocab_dict = {word:idx for (idx, word) in enumerate(model.get_words())}\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3093634 sha256=1ea9742ef714bfd9f6085cfa2a86a609850e73b76276477e3a7b7d76bda8add6\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRtwza13XRQ2"
      },
      "source": [
        "## Download pre-processed data\n",
        "gdd.download_file_from_google_drive(file_id='1PUiB33hgTsefasb3D2t920Gu2mINLd4C', dest_path='./preprocessed_data/preprocessed_data.zip', unzip=True)\n",
        "!rm ./preprocessed_data/preprocessed_data.zip\n",
        "\n",
        "\n",
        "\n",
        "trn = pd.read_csv('./preprocessed_data/trn_title.csv', delimiter = ',', names=['title','label'])\n",
        "tst = pd.read_csv('./preprocessed_data/tst_title.csv', delimiter = ',', names=['title','label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il9v7GNfLzeO",
        "outputId": "95fc0fea-c6c1-4478-9355-31f41f60ba02"
      },
      "source": [
        "print(trn.shape)\n",
        "print(tst.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31427, 2)\n",
            "(4492, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Ky3Aa1x2Su",
        "outputId": "3ded0035-564f-455c-99b5-ab474a151847"
      },
      "source": [
        "!pip install nltk==3.6\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from os.path import isfile\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "trn_sentences = tokenize(list(trn.title))\n",
        "trn_filtered_sentences = token_filter(trn_sentences,5)\n",
        "\n",
        "tst_sentences = tokenize(list(tst.title))\n",
        "tst_filtered_sentences = token_filter(tst_sentences,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/06/de681159e6750d0a215c2126e784504e177896afddf5a68cba42ebe42355/nltk-3.6-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (2019.12.20)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of vocab: 9495\n",
            "Length of vocab: 2169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfUqplGlL4qy",
        "outputId": "d3f684fb-4ddf-4f73-830e-ce95f24fb37d"
      },
      "source": [
        "print(trn_sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Texas', 'Moving', 'To', 'Ban', 'Jews', 'Muslims', 'and', 'Gays', 'from', 'Adopting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMs72p-1s6Me"
      },
      "source": [
        "with open('../preprocessed_embeddings/sg_train.txt', 'w') as f:\n",
        "    for title in trn_filtered_sentences:\n",
        "        for w in title: \n",
        "          f.write(\"%s \" % w)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "with open('../preprocessed_embeddings/sg_test.txt', 'w') as f:\n",
        "    for title in tst_filtered_sentences:\n",
        "        for w in title: \n",
        "          f.write(\"%s \" % w)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_sg = Skipgram('./preprocessed_data/sg_train.txt')\n",
        "model_sg.save_model(\"model_sg.bin\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WJkDvqyXehL"
      },
      "source": [
        "f1 = open(\"./preprocessed_data/sg_train.txt\", \"r\")\n",
        "f2 = open(\"./preprocessed_data/sg_test.txt\", \"r\")\n",
        "\n",
        "trn_embeddings = []\n",
        "tst_embeddings = [] \n",
        "\n",
        "for line in f1:\n",
        "    line_stripped = line.strip()  \n",
        "    trn_embeddings.append(model_sg.get_sentence_vector(line_stripped))\n",
        "\n",
        "f1.close()\n",
        "\n",
        "\n",
        "for line in f2:\n",
        "    line_stripped = line.strip()\n",
        "    tst_embeddings.append(model_sg.get_sentence_vector(line_stripped))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}