{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skipgram_embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "08_S3VGgs7u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8b4c46-8282-4a6a-a7a7-825baf5697c2"
      },
      "source": [
        "!pip install googledrivedownloader;\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ipX2QRwYms"
      },
      "source": [
        "def tokenize(sentences):\n",
        "\n",
        "    tokenized_senteces = list()\n",
        "    for s, sent in enumerate(sentences):\n",
        "\n",
        "        remove_punctuation = RegexpTokenizer(r\"\\w+\")\n",
        "        tokenized_sent = remove_punctuation.tokenize(sent)\n",
        "\n",
        "        for i, t in enumerate(tokenized_sent):\n",
        "            if t.isnumeric():\n",
        "                tokenized_sent[i] = \"<num>\"\n",
        "\n",
        "        tokenized_senteces.append(tokenized_sent)\n",
        "\n",
        "    return tokenized_senteces\n",
        "\n",
        "\n",
        "def token_filter(tokenized_sentences, thresh=5):\n",
        "    \n",
        "    words = list()\n",
        "    for t_sent in tokenized_sentences:\n",
        "        for word in t_sent:\n",
        "            words.append(word)\n",
        "\n",
        "    vocab = Vocabulary(words, unk_cutoff=thresh)\n",
        "    print(\"Length of vocab: {}\".format(len(vocab)))\n",
        "    filtered_sentences = list()\n",
        "\n",
        "    for t, tokenized_sent in enumerate(tokenized_sentences):\n",
        "        filtered_sent = list()\n",
        "        for word in tokenized_sent:\n",
        "            if vocab.lookup(word) == '<UNK>':\n",
        "                filtered_sent.append('<unk>')\n",
        "            else: \n",
        "                filtered_sent.append(word)\n",
        "        filtered_sentences.append(filtered_sent)\n",
        "\n",
        "    return filtered_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9_wpXWcrpxa",
        "outputId": "e5ca7d48-d91e-40fb-c51c-69f48418a72f"
      },
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "\n",
        "def Skipgram(filtered_sentences, ws=3, dim=50):\n",
        "    model = fasttext.train_unsupervised(filtered_sentences, model='skipgram', ws=ws, dim=dim, neg=5)\n",
        "    vocab_dict = {} \n",
        "    vocab_dict = {word:idx for (idx, word) in enumerate(model.get_words())}\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (56.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3092705 sha256=1f7ccfcd116b8f17520c7516a2f7c9ab55f9d1a820779ac360062451e5057d74\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRtwza13XRQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3f0795-36b6-42fc-a6e1-7d948d4a79ab"
      },
      "source": [
        "## Download pre-processed data\n",
        "gdd.download_file_from_google_drive(file_id='1PUiB33hgTsefasb3D2t920Gu2mINLd4C', dest_path='../preprocessed_data/preprocessed_data.zip', unzip=True)\n",
        "!rm ../preprocessed_data/preprocessed_data.zip\n",
        "\n",
        "\n",
        "\n",
        "trn = pd.read_csv('../preprocessed_data/trn_title.csv', delimiter = ',', names=['title','label'])\n",
        "tst = pd.read_csv('../preprocessed_data/tst_title.csv', delimiter = ',', names=['title','label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1PUiB33hgTsefasb3D2t920Gu2mINLd4C into ../preprocessed_data/preprocessed_data.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il9v7GNfLzeO",
        "outputId": "7981e7bf-0fe9-45f4-8f18-a48bc5b6875d"
      },
      "source": [
        "print(trn.shape)\n",
        "print(tst.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30986, 2)\n",
            "(4428, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Ky3Aa1x2Su",
        "outputId": "68bd4c80-afbf-46f7-d31e-ee5fd1e55a71"
      },
      "source": [
        "!pip install nltk==3.6\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from os.path import isfile\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "trn_sentences = tokenize(list(trn.title))\n",
        "trn_filtered_sentences = token_filter(trn_sentences,5)\n",
        "\n",
        "tst_sentences = tokenize(list(tst.title))\n",
        "tst_filtered_sentences = token_filter(tst_sentences,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.6 in /usr/local/lib/python3.7/dist-packages (3.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (2019.12.20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of vocab: 9445\n",
            "Length of vocab: 2106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfUqplGlL4qy",
        "outputId": "2c11c9a2-a7f9-4482-aae1-2b6d1cbec6ae"
      },
      "source": [
        "print(trn_sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['WATCH', 'Louis', 'C', 'K', 'NAILS', 'Trump', 'And', 'His', 'unk', 'Supporters', 'During', 'Appearance', 'On', 'Colbert']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMs72p-1s6Me"
      },
      "source": [
        "with open('../preprocessed_data/skipgram_train_lines.txt', 'w') as f:\n",
        "    for title in trn_filtered_sentences:\n",
        "        for w in title: \n",
        "          f.write(\"%s \" % w)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "with open('../preprocessed_data/skipgram_test_lines.txt', 'w') as f:\n",
        "    for title in tst_filtered_sentences:\n",
        "        for w in title: \n",
        "          f.write(\"%s \" % w)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "\n",
        "skipgram_model = Skipgram('../preprocessed_data/skipgram_train_lines.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WJkDvqyXehL"
      },
      "source": [
        "f1 = open(\"../preprocessed_data/skipgram_train_lines.txt\", \"r\")\n",
        "f2 = open(\"../preprocessed_data/skipgram_test_lines.txt\", \"r\")\n",
        "\n",
        "trn_embeddings = []\n",
        "tst_embeddings = [] \n",
        "\n",
        "for line in f1:\n",
        "    line_stripped = line.strip()  \n",
        "    trn_embeddings.append(skipgram_model.get_sentence_vector(line_stripped))\n",
        "\n",
        "f1.close()\n",
        "\n",
        "\n",
        "for line in f2:\n",
        "    line_stripped = line.strip()\n",
        "    tst_embeddings.append(skipgram_model.get_sentence_vector(line_stripped))\n",
        "\n",
        "f = open(\"../preprocessed_embeddings/skipgram_train_embeddings.pkl\",\"wb\")\n",
        "pickle.dump(trn_embeddings,f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../preprocessed_embeddings/skipgram_test_embeddings.pkl\",\"wb\")\n",
        "pickle.dump(tst_embeddings,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}