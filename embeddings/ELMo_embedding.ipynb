{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0336bdb9e57cd699596144781d3896191cc7f291091104ee557bbaee3fcafe96c",
   "display_name": "Python 3.8.8 64-bit ('ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install allennlp;\n",
    "!pip install googledrivedownloader;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ft/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from os.path import isfile\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "# Load ELMo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download weights and config file\n",
    "if not isfile(\"../downloads/elmo_2x4096_512_2048cnn_2xhighway_options.json\"):\n",
    "    url_options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "    filename, headers = urllib.request.urlretrieve(url_options_file, filename=\"../downloads/elmo_2x4096_512_2048cnn_2xhighway_options.json\")\n",
    "\n",
    "if not isfile(\"../downloads/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"):\n",
    "    url_weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "    filename, headers = urllib.request.urlretrieve(url_weight_file, filename=\"../downloads/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = '../downloads/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n",
    "weight_file = '../downloads/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load ELMo Token Embedder\n",
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file=weight_file, dropout=0).to(device)"
   ]
  },
  {
   "source": [
    "# Common functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "\n",
    "    tokenized_senteces = list()\n",
    "    for s, sent in enumerate(sentences):\n",
    "\n",
    "        remove_punctuation = RegexpTokenizer(r\"\\w+\")\n",
    "        tokenized_sent = remove_punctuation.tokenize(sent)\n",
    "\n",
    "        for i, t in enumerate(tokenized_sent):\n",
    "            if t.isnumeric():\n",
    "                tokenized_sent[i] = \"<num>\"\n",
    "\n",
    "        tokenized_senteces.append(tokenized_sent)\n",
    "\n",
    "    return tokenized_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter(tokenized_sentences, thresh=5):\n",
    "    \n",
    "    words = list()\n",
    "    for t_sent in tokenized_sentences:\n",
    "        for word in t_sent:\n",
    "            words.append(word)\n",
    "\n",
    "    vocab = Vocabulary(words, unk_cutoff=thresh)\n",
    "\n",
    "    filtered_sentences = list()\n",
    "\n",
    "    for t, tokenized_sent in enumerate(tokenized_sentences):\n",
    "        filtered_sent = list()\n",
    "        for word in tokenized_sent:\n",
    "            if vocab.lookup(word) == '<UNK>':\n",
    "                filtered_sent.append('<unk>')\n",
    "            else: \n",
    "                filtered_sent.append(word)\n",
    "        filtered_sentences.append(filtered_sent)\n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embedding(filtered_sentences, device=device):\n",
    "    vecs = list()\n",
    "    batch_size = 128\n",
    "    batches = math.ceil( len(filtered_sentences) / batch_size )\n",
    "    for i in tqdm(range(batches)):\n",
    "        character_ids = batch_to_ids(filtered_sentences[i * batch_size : (i + 1) * batch_size])\n",
    "        batch_embedding = elmo_embedder(character_ids.to(device))\n",
    "        batch_embedding = torch.mean(batch_embedding, dim=1)\n",
    "        vecs.extend(batch_embedding.cpu().detach().numpy())\n",
    "\n",
    "    return np.array(vecs)"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_csv('../preprocessed_data/trn_title.csv', delimiter = ',', names=['title','label'])\n",
    "tst = pd.read_csv('../preprocessed_data/tst_title.csv', delimiter = ',', names=['title','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sentences = tokenize(list(trn.title))\n",
    "trn_filtered_sentences = token_filter(trn_sentences,5)\n",
    "\n",
    "tst_sentences = tokenize(list(tst.title))\n",
    "tst_filtered_sentences = token_filter(tst_sentences,5)"
   ]
  },
  {
   "source": [
    "# Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_embedings = get_elmo_embedding(trn_filtered_sentences, device)\n",
    "\n",
    "f = open(\"../preprocessed_embeddings/elmo_trn.pkl\",\"wb\")\n",
    "pickle.dump(trn_embedings,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 36/36 [00:45<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "tst_embedings = get_elmo_embedding(tst_filtered_sentences, device)\n",
    "\n",
    "f = open(\"../preprocessed_embeddings/elmo_tst.pkl\",\"wb\")\n",
    "pickle.dump(tst_embedings,f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "# Download preprocessed embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading 1esvWZDtDMe-TUG7sR_U5N9QytebN4Cjy into ./preprocessed_embeddings/asd.zip... Done.\n",
      "Unzipping...Done.\n"
     ]
    }
   ],
   "source": [
    "## Download pre-processed embeddings\n",
    "gdd.download_file_from_google_drive(file_id='1esvWZDtDMe-TUG7sR_U5N9QytebN4Cjy', dest_path='../preprocessed_embeddings/elmo_embeddings.zip', unzip=True)\n",
    "!rm ./preprocessed_embeddings/elmo_embeddings.zip"
   ]
  }
 ]
}