{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0336bdb9e57cd699596144781d3896191cc7f291091104ee557bbaee3fcafe96c",
   "display_name": "Python 3.8.8 64-bit ('ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(torch.randn(5))"
   ]
  },
  {
   "source": [
    "# Common functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trn_dataloader, optimizer, criterion, device=torch.device('cpu')):\n",
    "    total_batch, correct_pred, total_sample = 0, 0, 0\n",
    "    model.train()\n",
    "\n",
    "    for batch, label in trn_dataloader:\n",
    "        total_batch += 1\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        trn_loss = loss.data.item()\n",
    "        pred_label = output.argmax(dim=1)\n",
    "        correct_pred += (pred_label == label).sum()\n",
    "        total_sample += label.size()[0]\n",
    "\n",
    "        if total_batch % 50 == 0:\n",
    "            print(f\"#{total_batch}: trn loss = {trn_loss:.4f} | trn acc = {(correct_pred/total_sample):.4f}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion, device=torch.device('cpu')):\n",
    "    total_loss, total_batch = 0, 0\n",
    "    correct_pred, total_sample = 0, 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch, label in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(batch)\n",
    "\n",
    "        total_batch += 1\n",
    "        loss = criterion(output, label)\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "        pred_label = output.argmax(dim=1)\n",
    "        correct_pred += (pred_label == label).sum()\n",
    "        total_sample += label.size()[0]\n",
    "\n",
    "    avg_loss = total_loss / total_batch\n",
    "    acc = correct_pred.item() / total_sample\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn = pickle.load(open(\"../preprocessed_embeddings/elmo_trn_title_labels.pkl\", \"rb\"))\n",
    "y_val = pickle.load(open(\"../preprocessed_embeddings/elmo_val_title_labels.pkl\", \"rb\"))\n",
    "y_tst = pickle.load(open(\"../preprocessed_embeddings/elmo_tst_title_labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn = pickle.load(open(\"../preprocessed_embeddings/elmo_trn_title.pkl\", \"rb\")).tolist()\n",
    "x_val = pickle.load(open(\"../preprocessed_embeddings/elmo_val_title.pkl\", \"rb\")).tolist()\n",
    "x_tst = pickle.load(open(\"../preprocessed_embeddings/elmo_tst_title.pkl\", \"rb\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "### Training set\n",
    "trn_dataset = []\n",
    "for i in range(len(x_trn)):\n",
    "    trn_dataset.append((torch.tensor(x_trn[i]), y_trn[i]))\n",
    "\n",
    "del x_trn\n",
    "del y_trn\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size)\n",
    "\n",
    "### Validation set\n",
    "val_dataset = []\n",
    "for i in range(len(x_val)):\n",
    "    val_dataset.append((torch.tensor(x_val[i]), y_val[i]))\n",
    "\n",
    "del x_val\n",
    "del y_val\n",
    "val_dataloader = DataLoader(val_dataset, batch_size)\n",
    "\n",
    "### Test set\n",
    "tst_dataset = []\n",
    "for i in range(len(x_tst)):\n",
    "    tst_dataset.append((torch.tensor(x_tst[i]), y_tst[i]))\n",
    "\n",
    "del x_tst\n",
    "del y_tst\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size)"
   ]
  },
  {
   "source": [
    "# Convolutional Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, drop_rate=0.0, kernel_size=4, embed_size=1024, class_size=2):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # ---------------------------------\n",
    "        # Configuration\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.filter = nn.Conv1d(embed_size, embed_size, kernel_size)\n",
    "        self.fc = nn.Linear(embed_size, class_size)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Batch -> B x L x E\n",
    "        x = batch.permute(0,2,1) # x -> B x E x L\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.filter(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.max(x, dim=2)[0]\n",
    "\n",
    "        out = self.fc(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0\n",
      "\n",
      "#50: trn loss = 0.0015 | trn acc = 0.9567\n",
      "#100: trn loss = 0.0004 | trn acc = 0.9776\n",
      "#150: trn loss = 0.0019 | trn acc = 0.9831\n",
      "#200: trn loss = 0.0377 | trn acc = 0.9867\n",
      "\n",
      "trn loss = 0.05926284155394296 trn acc = 0.9781514232233912 val loss = 0.16791883727649162 val acc = 0.9443126623743364\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "#50: trn loss = 0.0012 | trn acc = 0.9955\n",
      "#100: trn loss = 0.0003 | trn acc = 0.9977\n",
      "#150: trn loss = 0.0002 | trn acc = 0.9982\n",
      "#200: trn loss = 0.0004 | trn acc = 0.9985\n",
      "\n",
      "trn loss = 0.012550897891626327 trn acc = 0.9948363777189698 val loss = 0.047145395779391815 val acc = 0.9818140743250875\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "#50: trn loss = 0.0000 | trn acc = 0.9991\n",
      "#100: trn loss = 0.0001 | trn acc = 0.9995\n",
      "#150: trn loss = 0.0000 | trn acc = 0.9996\n",
      "#200: trn loss = 0.0000 | trn acc = 0.9997\n",
      "\n",
      "trn loss = 0.008424548098660375 trn acc = 0.9964177370425353 val loss = 0.042913020557567245 val acc = 0.9871230091494408\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "#50: trn loss = 0.0000 | trn acc = 0.9992\n",
      "#100: trn loss = 0.0000 | trn acc = 0.9995\n",
      "#150: trn loss = 0.0000 | trn acc = 0.9996\n",
      "#200: trn loss = 0.0000 | trn acc = 0.9997\n",
      "\n",
      "trn loss = 0.0029281657996935197 trn acc = 0.9989027302652811 val loss = 0.023602413031169688 val acc = 0.99310968033435\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "#50: trn loss = 0.0000 | trn acc = 0.9995\n",
      "#100: trn loss = 0.0000 | trn acc = 0.9998\n",
      "#150: trn loss = 0.0000 | trn acc = 0.9998\n",
      "#200: trn loss = 0.0000 | trn acc = 0.9999\n",
      "\n",
      "trn loss = 0.0012744997251352395 trn acc = 0.9996127283289228 val loss = 0.018285365798471634 val acc = 0.9951428894160171\n"
     ]
    }
   ],
   "source": [
    "for e in range(5):\n",
    "    print(f'Epoch {e}\\n') if e == 0 else print(f'\\nEpoch {e}\\n')\n",
    "    model = train(model, trn_dataloader, optimizer, criterion, device)\n",
    "    trn_loss, trn_acc = eval(model, trn_dataloader, criterion, device)\n",
    "    val_loss, val_acc = eval(model, val_dataloader, criterion, device)\n",
    "    print(f\"\\ntrn loss = {trn_loss} trn acc = {trn_acc} val loss = {val_loss} val acc = {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Loss: 0.018285365798471634\nValidation Acc: 0.9951428894160171\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = eval(model, val_dataloader, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Loss: 0.032086542630161474\nTest Acc: 0.989159891598916\n"
     ]
    }
   ],
   "source": [
    "tst_loss, tst_acc = eval(model, tst_dataloader, criterion, device)\n",
    "print(f\"Test Loss: {tst_loss}\")\n",
    "print(f\"Test Acc: {tst_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      2286\n           1       1.00      0.98      0.99      2142\n\n    accuracy                           0.99      4428\n   macro avg       0.99      0.99      0.99      4428\nweighted avg       0.99      0.99      0.99      4428\n\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "true_labels = []\n",
    "for batch, labels in tst_dataloader:\n",
    "    batch_predictions = model(batch.to(device)).argmax(dim=1).to(torch.device('cpu')).tolist()\n",
    "    predictions.extend(batch_predictions)\n",
    "    true_labels.extend(labels.tolist())\n",
    "    \n",
    "print(classification_report(true_labels, predictions))"
   ]
  }
 ]
}