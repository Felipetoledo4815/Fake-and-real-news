{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T7Krt3hr1CT",
        "outputId": "73e3775e-e954-4ac9-dfee-7e331fa58793"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOtN73NQrMTK"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4vm0TFNrTS8",
        "outputId": "fbf08797-805d-40f8-aea4-503d64db8517"
      },
      "source": [
        "seed = 1\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(torch.randn(5))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jrPYw5frbN0"
      },
      "source": [
        "y_trn = pickle.load(open(\"./drive/MyDrive/elmo_embeddings/elmo_trn_title_labels.pkl\", \"rb\"))\n",
        "y_val = pickle.load(open(\"./drive/MyDrive/elmo_embeddings/elmo_val_title_labels.pkl\", \"rb\"))\n",
        "y_tst = pickle.load(open(\"./drive/MyDrive/elmo_embeddings/elmo_tst_title_labels.pkl\", \"rb\"))"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5In0jsNrcwY"
      },
      "source": [
        "x_trn = pickle.load(open(\"./drive/MyDrive/elmo_embeddings/elmo_trn_title.pkl\", \"rb\")).tolist()\n",
        "x_val = pickle.load(open(\"./drive/MyDrive/elmo_embeddings/elmo_val_title.pkl\", \"rb\")).tolist()\n",
        "x_tst = pickle.load(open(\"./drive/MyDrive/elmo_embeddings/elmo_tst_title.pkl\", \"rb\")).tolist()"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJpmKcowruMn"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "## Training set\n",
        "trn_dataset = []\n",
        "for i in range(len(x_trn)):\n",
        "    trn_dataset.append((torch.tensor(x_trn[i]), y_trn[i]))\n",
        "\n",
        "del x_trn\n",
        "del y_trn\n",
        "trn_dataloader = DataLoader(trn_dataset, batch_size)\n",
        "\n",
        "### Validation set\n",
        "val_dataset = []\n",
        "for i in range(len(x_val)):\n",
        "    val_dataset.append((torch.tensor(x_val[i]), y_val[i]))\n",
        "\n",
        "del x_val\n",
        "del y_val\n",
        "val_dataloader = DataLoader(val_dataset, batch_size)\n",
        "\n",
        "### Test set\n",
        "tst_dataset = []\n",
        "for i in range(len(x_tst)):\n",
        "    tst_dataset.append((torch.tensor(x_tst[i]), y_tst[i]))\n",
        "\n",
        "del x_tst\n",
        "del y_tst\n",
        "tst_dataloader = DataLoader(tst_dataset, batch_size)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_O22dDDsUIP",
        "outputId": "07f3ec0a-8678-4e5e-d86b-5ac0e26db96c"
      },
      "source": [
        "val_iter = iter(val_dataloader)\n",
        "\n",
        "i = next(val_iter)\n",
        "\n",
        "print(i[0].shape) # B x L x E \n",
        "print(i[1].shape)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 7, 1024])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpGYzLzZDuzu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ttEnkxWs1a_"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_size, output_size, num_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.embedding_dim = embedding_dim \n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size \n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(self.hidden_size, self.output_size) # fully connected layer \n",
        "  def init_state(self, batch_size): # create dummy state for (h0, c0)\n",
        "    return (torch.zeros(self.num_layers, batch_size, self.hidden_size), torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    h0, c0 = self.init_state(batch_size)\n",
        "    h0 = h0.to(device)\n",
        "    c0 = c0.to(device)\n",
        "    output, hidden = self.lstm(x, (h0, c0))\n",
        "    fc_output = self.fc(output)\n",
        "    print(fc_output.shape)\n",
        "    return fc_output"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "LH7ZaESkLLdZ",
        "outputId": "01901451-6353-4829-9c4c-3fa807ce16c3"
      },
      "source": [
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
        "grad_clip = 1.0\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def batch_train(batch, labels, model, optimizer):\n",
        "    model.train()\n",
        "    texts = batch.to(device)\n",
        "    h0, c0 = model.init_state(batch_size=texts.shape[0])\n",
        "    h0 = h0.to(device)\n",
        "    c0 = c0.to(device)\n",
        "    \n",
        "    predictions = model(texts)\n",
        "    targets = labels.to(device)\n",
        "    cost_function = nn.CrossEntropyLoss()\n",
        "    h0 = h0.detach() \n",
        "    c0 = c0.detach()\n",
        "\n",
        "    # Cross Entropy Loss \n",
        "    loss = cost_function(predictions, targets.unsqueeze(0))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    clip_grad_norm(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    return model, loss.item()\n",
        "\n",
        "def eval(data_loader, model):\n",
        "    # set in the eval model, which will trun off the features only used for training, such as droput\n",
        "    model.eval()\n",
        "    # records\n",
        "    val_loss = 0\n",
        "    val_batch = 0 \n",
        "    # iterate all the mini batches for evaluation\n",
        "    for batch, label in data_loader:\n",
        "        texts = batch.to(device)\n",
        "        targets = label.to(device)\n",
        "        # Forward: prediction\n",
        "        h0, c0 = model.init_state(batch_size=texts.shape[0])\n",
        "        h0 = h0.to(device)\n",
        "        c0 = c0.to(device)\n",
        "        predictions = model(texts)\n",
        "        cost_function = nn.CrossEntropyLoss()\n",
        "        h0 = h0.detach() \n",
        "        c0 = c0.detach()\n",
        "        loss = cost_function(predictions, targets.unsqueeze(0))\n",
        "\n",
        "        val_batch += 1\n",
        "        val_loss += loss.item()\n",
        "    return (val_loss/val_batch)\n",
        "\n",
        "model = RNN(embedding_dim=1024, hidden_size=128, output_size=1).to(device)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "epoch, val_step = 10, 250\n",
        "total_batch = 0\n",
        "best_val_loss = 20\n",
        "for e in range(epoch):\n",
        "    for batch, label in trn_dataloader:\n",
        "        total_batch += 1\n",
        "        # Update parameters with one batch\n",
        "        model, loss = batch_train(batch, label, model, optimizer)\n",
        "        # Compute validation loss after each val_step\n",
        "        if total_batch % val_step == 0:\n",
        "            val_loss = eval(val_loader, model)\n",
        "            if val_loss < best_val_loss:\n",
        "              best_val_loss = val_loss\n",
        "              torch.save(model.state_dict(), \"./drive/MyDrive/best_models/best_rnn_weights.pth\")\n",
        "              \n",
        "            print(\"Epoch: {}, Batch: {}, Current val loss: {}. Best val loss: {}\".format(e, b, val_loss, best_val_loss))\n"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 6, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-f2b20bf39188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Update parameters with one batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Compute validation loss after each val_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-231-f2b20bf39188>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(batch, labels, model, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Cross Entropy Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m         raise ValueError(\n\u001b[0;32m-> 2385\u001b[0;31m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (16) to match target batch_size (1)."
          ]
        }
      ]
    }
  ]
}