{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skipgram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08_S3VGgs7u-",
        "outputId": "dc9b4829-5e20-41c2-9254-75e4b83483d4"
      },
      "source": [
        "!pip install googledrivedownloader;\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(df.index)\n",
        "    m = len(df.index)\n",
        "    train_end = int(train_percent * m)\n",
        "    validate_end = int(validate_percent * m) + train_end\n",
        "    train = df.iloc[perm[:train_end]]\n",
        "    validate = df.iloc[perm[train_end:validate_end]]\n",
        "    test = df.iloc[perm[validate_end:]]\n",
        "    return train, validate, test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwc_pLZgtHt6",
        "outputId": "79663b3c-294e-434f-c0dc-59187f7fd975"
      },
      "source": [
        "fake = pd.read_csv('Fake.csv', delimiter = ',')\n",
        "fake['label']= 0\n",
        "\n",
        "print(\"# fake news = \" + str(len(fake)) + '\\n')\n",
        "\n",
        "unique_texts = fake['text'].value_counts().sum()\n",
        "empty_texts = fake[fake['text'] == ' '].append(fake[fake['text'] == '  ']).value_counts().sum()\n",
        "print(\"# unique texts = \" + str(unique_texts))\n",
        "print(\"# of empty texts = \" + str(empty_texts))\n",
        "print(\"Total texts = \" + str(unique_texts - empty_texts) + '\\n')\n",
        "\n",
        "unique_titles = fake['title'].value_counts().sum()\n",
        "empty_titles = fake[fake['title'] == ' '].append(fake[fake['title'] == '  ']).value_counts().sum()\n",
        "print(\"# unique titles = \" + str(unique_titles))\n",
        "print(\"# of empty titles = \" + str(empty_titles))\n",
        "print(\"Total texts = \" + str(unique_titles - empty_titles))\n",
        "\n",
        "fake = fake.drop(columns=['subject','date'])\n",
        "fake_trn, fake_val, fake_tst = train_validate_test_split(fake, train_percent=0.7, validate_percent=0.2, seed=1)\n",
        "\n",
        "true = pd.read_csv('True.csv', delimiter = ',')\n",
        "true['label']= 1\n",
        "\n",
        "print(\"# true news = \" + str(len(true)) + '\\n')\n",
        "\n",
        "unique_texts = true['text'].value_counts().sum()\n",
        "empty_texts = true[true['text'] == ' '].append(true[true['text'] == '  ']).value_counts().sum()\n",
        "print(\"# unique texts = \" + str(unique_texts))\n",
        "print(\"# of empty texts = \" + str(empty_texts))\n",
        "print(\"Total texts = \" + str(unique_texts - empty_texts) + '\\n')\n",
        "\n",
        "unique_titles = true['title'].value_counts().sum()\n",
        "empty_titles = true[true['title'] == ' '].append(true[true['title'] == '  ']).value_counts().sum()\n",
        "print(\"# unique titles = \" + str(unique_titles))\n",
        "print(\"# of empty titles = \" + str(empty_titles))\n",
        "print(\"Total texts = \" + str(unique_titles - empty_titles))\n",
        "\n",
        "true = true.drop(columns=['subject','date'])\n",
        "true_trn, true_val, true_tst = train_validate_test_split(true, train_percent=0.7, validate_percent=0.2, seed=1)\n",
        "\n",
        "df_trn, df_val, df_tst = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "df_trn = true_trn.append(fake_trn).sample(frac=1).reset_index(drop=True)\n",
        "df_val = true_val.append(fake_val).sample(frac=1).reset_index(drop=True)\n",
        "df_tst = true_tst.append(fake_tst).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(\"Training set: \" + str(len(df_trn)))\n",
        "print(\"Validation set: \" + str(len(df_val)))\n",
        "print(\"Test set: \" + str(len(df_tst)))\n",
        "\n",
        "\n",
        "df_trn.drop(columns=['text']).to_csv('./preprocessed_data/trn_title.csv', header=False, index=False)\n",
        "df_val.drop(columns=['text']).to_csv('./preprocessed_data/val_title.csv', header=False, index=False)\n",
        "df_tst.drop(columns=['text']).to_csv('./preprocessed_data/tst_title.csv', header=False, index=False)\n",
        "\n",
        "df_trn.drop(columns=['title']).to_csv('./preprocessed_data/trn_text.csv', header=False, index=False)\n",
        "df_val.drop(columns=['title']).to_csv('./preprocessed_data/val_text.csv', header=False, index=False)\n",
        "df_tst.drop(columns=['title']).to_csv('./preprocessed_data/tst_text.csv', header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# fake news = 23481\n",
            "\n",
            "# unique texts = 23481\n",
            "# of empty texts = 630\n",
            "Total texts = 22851\n",
            "\n",
            "# unique titles = 23481\n",
            "# of empty titles = 0\n",
            "Total texts = 23481\n",
            "# true news = 21417\n",
            "\n",
            "# unique texts = 21417\n",
            "# of empty texts = 1\n",
            "Total texts = 21416\n",
            "\n",
            "# unique titles = 21417\n",
            "# of empty titles = 0\n",
            "Total texts = 21417\n",
            "Training set: 31427\n",
            "Validation set: 8979\n",
            "Test set: 4492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ipX2QRwYms"
      },
      "source": [
        "def tokenize(sentences):\n",
        "\n",
        "    tokenized_senteces = list()\n",
        "    for s, sent in enumerate(sentences):\n",
        "\n",
        "        remove_punctuation = RegexpTokenizer(r\"\\w+\")\n",
        "        tokenized_sent = remove_punctuation.tokenize(sent)\n",
        "\n",
        "        for i, t in enumerate(tokenized_sent):\n",
        "            if t.isnumeric():\n",
        "                tokenized_sent[i] = \"<num>\"\n",
        "\n",
        "        tokenized_senteces.append(tokenized_sent)\n",
        "\n",
        "    return tokenized_senteces\n",
        "\n",
        "\n",
        "def token_filter(tokenized_sentences, thresh=5):\n",
        "    \n",
        "    words = list()\n",
        "    for t_sent in tokenized_sentences:\n",
        "        for word in t_sent:\n",
        "            words.append(word)\n",
        "\n",
        "    vocab = Vocabulary(words, unk_cutoff=thresh)\n",
        "\n",
        "    filtered_sentences = list()\n",
        "\n",
        "    for t, tokenized_sent in enumerate(tokenized_sentences):\n",
        "        filtered_sent = list()\n",
        "        for word in tokenized_sent:\n",
        "            if vocab.lookup(word) == '<UNK>':\n",
        "                filtered_sent.append('<unk>')\n",
        "            else: \n",
        "                filtered_sent.append(word)\n",
        "        filtered_sentences.append(filtered_sent)\n",
        "\n",
        "    return filtered_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvbB5_Y-xaUG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9_wpXWcrpxa",
        "outputId": "0de00600-c251-4e6e-b4eb-c3c95571f1a8"
      },
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "\n",
        "def Skipgram(filtered_sentences, ws=3, dim=50):\n",
        "    model = fasttext.train_unsupervised(filtered_sentences, model='skipgram', ws=ws, dim=dim, neg=5)\n",
        "    vocab_dict = {} \n",
        "    vocab_dict = {word:idx for (idx, word) in enumerate(model.get_words())}\n",
        "    return model.get_output_matrix(), vocab_dict\n",
        "\n",
        "\n",
        "trn = pd.read_csv('./preprocessed_data/trn_title.csv', delimiter = ',', names=['title','label'])\n",
        "tst = pd.read_csv('./preprocessed_data/tst_title.csv', delimiter = ',', names=['title','label'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Ky3Aa1x2Su",
        "outputId": "5d7963f4-5155-4ff9-e665-58cd0b38385c"
      },
      "source": [
        "!pip install nltk==3.6\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from os.path import isfile\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "trn_sentences = tokenize(list(trn.title))\n",
        "trn_filtered_sentences = token_filter(trn_sentences,5)\n",
        "\n",
        "tst_sentences = tokenize(list(tst.title))\n",
        "tst_filtered_sentences = token_filter(tst_sentences,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.6 in /usr/local/lib/python3.7/dist-packages (3.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6) (4.41.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMs72p-1s6Me"
      },
      "source": [
        "with open('./preprocessed_data/sg_train.txt', 'w') as f:\n",
        "    for title in trn_filtered_sentences:\n",
        "        for w in title: \n",
        "          f.write(\"%s \" % w)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "with open('./preprocessed_data/sg_test.txt', 'w') as f:\n",
        "    for title in tst_filtered_sentences:\n",
        "        for w in title: \n",
        "          f.write(\"%s \" % w)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trn_embedings, vocab_trn = Skipgram('./preprocessed_data/sg_train.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f = open(\"./preprocessed_embeddings/sg_trn.pkl\",\"wb\")\n",
        "pickle.dump(trn_embedings,f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "tst_embedings, vocab_tst = Skipgram('./preprocessed_data/sg_test.txt')\n",
        "\n",
        "f = open(\"./preprocessed_embeddings/sg_tst.pkl\",\"wb\")\n",
        "pickle.dump(tst_embedings,f)\n",
        "f.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}