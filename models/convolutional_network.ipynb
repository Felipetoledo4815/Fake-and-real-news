{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0336bdb9e57cd699596144781d3896191cc7f291091104ee557bbaee3fcafe96c",
   "display_name": "Python 3.8.8 64-bit ('ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(torch.randn(5))"
   ]
  },
  {
   "source": [
    "# Common functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trn_dataloader, optimizer, criterion, device=torch.device('cpu')):\n",
    "    total_batch, correct_pred, total_sample = 0, 0, 0\n",
    "    model.train()\n",
    "\n",
    "    for batch, label in trn_dataloader:\n",
    "        total_batch += 1\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        trn_loss = loss.data.item()\n",
    "        pred_label = output.argmax(dim=1)\n",
    "        correct_pred += (pred_label == label).sum()\n",
    "        total_sample += label.size()[0]\n",
    "\n",
    "        if total_batch % 50 == 0:\n",
    "            print(f\"#{total_batch}: trn loss = {trn_loss:.4f} | trn acc = {(correct_pred/total_sample):.4f}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion, device=torch.device('cpu')):\n",
    "    total_loss, total_batch = 0, 0\n",
    "    correct_pred, total_sample = 0, 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch, label in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(batch)\n",
    "\n",
    "        total_batch += 1\n",
    "        loss = criterion(output, label)\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "        pred_label = output.argmax(dim=1)\n",
    "        correct_pred += (pred_label == label).sum()\n",
    "        total_sample += label.size()[0]\n",
    "\n",
    "    avg_loss = total_loss / total_batch\n",
    "    acc = correct_pred.item() / total_sample\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn = pickle.load(open(\"../preprocessed_embeddings/elmo_trn_title_labels.pkl\", \"rb\"))\n",
    "y_val = pickle.load(open(\"../preprocessed_embeddings/elmo_val_title_labels.pkl\", \"rb\"))\n",
    "y_tst = pickle.load(open(\"../preprocessed_embeddings/elmo_tst_title_labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn = pickle.load(open(\"../preprocessed_embeddings/elmo_trn_title.pkl\", \"rb\")).tolist()\n",
    "x_val = pickle.load(open(\"../preprocessed_embeddings/elmo_val_title.pkl\", \"rb\")).tolist()\n",
    "x_tst = pickle.load(open(\"../preprocessed_embeddings/elmo_tst_title.pkl\", \"rb\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "### Training set\n",
    "trn_dataset = []\n",
    "for i in range(len(x_trn)):\n",
    "    trn_dataset.append((torch.tensor(x_trn[i]), y_trn[i]))\n",
    "\n",
    "del x_trn\n",
    "del y_trn\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size)\n",
    "\n",
    "### Validation set\n",
    "val_dataset = []\n",
    "for i in range(len(x_val)):\n",
    "    val_dataset.append((torch.tensor(x_val[i]), y_val[i]))\n",
    "\n",
    "del x_val\n",
    "del y_val\n",
    "val_dataloader = DataLoader(val_dataset, batch_size)\n",
    "\n",
    "### Test set\n",
    "tst_dataset = []\n",
    "for i in range(len(x_tst)):\n",
    "    tst_dataset.append((torch.tensor(x_tst[i]), y_tst[i]))\n",
    "\n",
    "del x_tst\n",
    "del y_tst\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size)"
   ]
  },
  {
   "source": [
    "# Convolutional Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, drop_rate=0.0, kernel_size=4, embed_size=1024, class_size=2):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # ---------------------------------\n",
    "        # Configuration\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.filter = nn.Conv1d(embed_size, embed_size, kernel_size)\n",
    "        self.fc = nn.Linear(embed_size, class_size)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Batch -> B x L x E\n",
    "        x = batch.permute(0,2,1) # x -> B x E x L\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.filter(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.max(x, dim=2)[0]\n",
    "\n",
    "        out = self.fc(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0\n",
      "\n",
      "#50: trn loss = 0.0012 | trn acc = 0.9720\n",
      "#100: trn loss = 0.0005 | trn acc = 0.9852\n",
      "#150: trn loss = 0.0022 | trn acc = 0.9882\n",
      "#200: trn loss = 0.0304 | trn acc = 0.9906\n",
      "\n",
      "trn loss = 0.05928844698463849 trn acc = 0.9781514232233912 val loss = 0.16929941649037314 val acc = 0.9428442335931323\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "#50: trn loss = 0.0014 | trn acc = 0.9955\n",
      "#100: trn loss = 0.0005 | trn acc = 0.9977\n",
      "#150: trn loss = 0.0003 | trn acc = 0.9982\n",
      "#200: trn loss = 0.0005 | trn acc = 0.9985\n",
      "\n",
      "trn loss = 0.015971828409890573 trn acc = 0.9936745627057381 val loss = 0.0568739628901832 val acc = 0.9793290410030498\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "#50: trn loss = 0.0000 | trn acc = 0.9991\n",
      "#100: trn loss = 0.0001 | trn acc = 0.9995\n",
      "#150: trn loss = 0.0000 | trn acc = 0.9996\n",
      "#200: trn loss = 0.0000 | trn acc = 0.9997\n",
      "\n",
      "trn loss = 0.011722615522003675 trn acc = 0.9951913767507907 val loss = 0.053237875625661346 val acc = 0.9830565909861064\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "#50: trn loss = 0.0000 | trn acc = 0.9994\n",
      "#100: trn loss = 0.0000 | trn acc = 0.9996\n",
      "#150: trn loss = 0.0000 | trn acc = 0.9997\n",
      "#200: trn loss = 0.0000 | trn acc = 0.9998\n",
      "\n",
      "trn loss = 0.0034712901367681135 trn acc = 0.9985154585942039 val loss = 0.024227925736506353 val acc = 0.992770812154072\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "#50: trn loss = 0.0000 | trn acc = 0.9994\n",
      "#100: trn loss = 0.0000 | trn acc = 0.9996\n",
      "#150: trn loss = 0.0000 | trn acc = 0.9997\n",
      "#200: trn loss = 0.0000 | trn acc = 0.9998\n",
      "\n",
      "trn loss = 0.001464086858061593 trn acc = 0.9994513651326405 val loss = 0.019983101046919468 val acc = 0.9941262848751835\n"
     ]
    }
   ],
   "source": [
    "for e in range(5):\n",
    "    print(f'Epoch {e}\\n') if e == 0 else print(f'\\nEpoch {e}\\n')\n",
    "    model = train(model, trn_dataloader, optimizer, criterion, device)\n",
    "    trn_loss, trn_acc = eval(model, trn_dataloader, criterion, device)\n",
    "    val_loss, val_acc = eval(model, val_dataloader, criterion, device)\n",
    "    print(f\"\\ntrn loss = {trn_loss} trn acc = {trn_acc} val loss = {val_loss} val acc = {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Loss: 0.019983101046919468\nValidation Acc: 0.9941262848751835\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = eval(model, val_dataloader, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Loss: 0.038260751161850036\nTest Acc: 0.9866757000903342\n"
     ]
    }
   ],
   "source": [
    "tst_loss, tst_acc = eval(model, tst_dataloader, criterion, device)\n",
    "print(f\"Test Loss: {tst_loss}\")\n",
    "print(f\"Test Acc: {tst_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}